{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u2g0MDvGcFJ"
      },
      "source": [
        "Practical Session - Transformers\n",
        "---\n",
        "\n",
        "### Goal  \n",
        "The goal of this session is to implement a standard Transformer in PyTorch from scratch.\n",
        "\n",
        "### Task  \n",
        "As a sample problem, we will focus on sorting a list of digits from 1 to 20.\n",
        "\n",
        "Example:\n",
        "Source Sequence: `[19, 7, 2, 9, 18]`\n",
        "Target Output:   `[2, 7, 9, 18, 19]`\n",
        "\n",
        "### Outline\n",
        "1. Embed the tokens/numbers into vectors\n",
        "2. The `Transformer` layer\n",
        "- 2.1. Implement the self-attention layer, as seen in class\n",
        "- 2.2 Integrate the self-attention layer in a transformer layer\n",
        "3. Implement a `Transformer` network\n",
        "4. Train the network!\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9GtTjNNGcFX"
      },
      "source": [
        "---\n",
        "#### 0. Generating data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpO8hbBGGcFY",
        "outputId": "61a3be13-2024-4d0e-fc59-9c876e591405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example source sequence and target:\n",
            "--------------------------------------------------\n",
            "Input Sequence:         [5, 18, 12, 14, 18]\n",
            "Expected Sorted Output: [5, 12, 14, 18, 18]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 20  # Numbers from 1 to vocab_size\n",
        "SEQ_LENGTH = 5   # Sequence length\n",
        "\n",
        "def generate_data(batch_size, seq_length, vocab_size):\n",
        "    \"\"\"\n",
        "    Generates random sequences of integers and their sorted counterparts.\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): Number of sequences to generate.\n",
        "        seq_length (int): Length of each sequence.\n",
        "        vocab_size (int): Maximum integer value (exclusive) for the sequence elements.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[torch.Tensor, torch.Tensor]:\n",
        "            - src (torch.Tensor): A tensor of shape (batch_size, seq_length) containing\n",
        "              random integers in the range [1, vocab_size).\n",
        "            - tgt (torch.Tensor): A tensor of shape (batch_size, seq_length) containing\n",
        "              the sorted version of each sequence in `src`.\n",
        "    \"\"\"\n",
        "    src = torch.randint(1, vocab_size, (batch_size, seq_length))\n",
        "    tgt = torch.sort(src, dim=1)[0]\n",
        "    return src, tgt\n",
        "\n",
        "source, target = generate_data(1, SEQ_LENGTH, VOCAB_SIZE)\n",
        "print(\"Example source sequence and target:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Input Sequence:         {source.tolist()[0]}\")\n",
        "print(f\"Expected Sorted Output: {target.tolist()[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdRk_8fkGcFe"
      },
      "source": [
        "---\n",
        "### Step 1: Embed the tokens into vectors\n",
        "\n",
        "First step is to transform the input integers into vectors of a fixed dimension `d`\n",
        "##### How to do this:  \n",
        "1. **Token Embeddings**: Each input token (integer index) is mapped to a high-dimensional vector using [`torch.nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html).  \n",
        "2. **Positional Encoding**: Instead of the classical sine-cosine positional encodings, we simply use a learnable vector for each position in the sequence, again using `torch.nn.Embedding`.\n",
        "3. **Summation**: The final embedding is the sum of token embeddings and positional encodings.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBysIdnsGcFf",
        "outputId": "3897b282-d071-462a-bfae-91c8daf60311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedded batch of sequences [[16, 7, 10, 5, 15]] into:\n",
            "Tensor of shape  torch.Size([1, 5, 3])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.4250,  0.4345, -2.6929],\n",
              "         [ 1.7109,  0.4940,  0.5560],\n",
              "         [-1.0345, -0.0189,  1.0782],\n",
              "         [ 0.3897,  0.8772, -0.4300],\n",
              "         [ 0.1627, -0.4316, -0.0877]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class IntegerSequenceEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Embedding module that combines token embeddings with positional encodings.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): Size of the vocabulary (number of unique tokens).\n",
        "        embed_dim (int): Dimension of the embeddings.\n",
        "        seq_length (int): Sequence length.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=20, embed_dim=16, seq_length=5):\n",
        "        super().__init__()\n",
        "        # embedding layer for the tokens (numbers):\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        # embedding layer for the positions:\n",
        "        self.positional_embedding = nn.Embedding(seq_length, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the embedding module.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length),\n",
        "                              containing integer token indices.\n",
        "        Returns:\n",
        "            torch.Tensor: Embedded tensor of shape (batch_size, seq_length, embed_dim).\n",
        "        \"\"\"\n",
        "        # Token embedding\n",
        "        x = self.token_embedding(x)  # Shape: (batch_size, seq_length, embed_dim)\n",
        "        # Positional encoding\n",
        "        positions = torch.arange(x.shape[1]).unsqueeze(0)              # Shape: (1, seq_length)\n",
        "        x = x + self.positional_embedding(positions)\n",
        "\n",
        "        return x\n",
        "\n",
        "embedding_layer = IntegerSequenceEmbedding(vocab_size=21, embed_dim=3, seq_length=SEQ_LENGTH)\n",
        "src, _ = generate_data(batch_size=1, seq_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE)\n",
        "embedded_src = embedding_layer(src)\n",
        "print(f\"Embedded batch of sequences {src.tolist()} into:\")\n",
        "print(f\"Tensor of shape  {embedded_src.shape}\")\n",
        "embedded_src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhM1Gn2LGcFh"
      },
      "source": [
        "---\n",
        "### Step 2: The `Transformer` layer\n",
        "\n",
        "#### 2.1. Implement the self-attention layer, as seen in class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Wgf--26GcFh",
        "outputId": "d2177153-8133-410e-9b33-769c9b008c34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 10, 128])\n"
          ]
        }
      ],
      "source": [
        "class SingleHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)  # Query projection\n",
        "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)  # Key projection\n",
        "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)  # Value projection\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x: (batch_size, seq_length, embed_dim)\n",
        "        # TODO: Complete this function\n",
        "        Q = self.W_q(x)  # (batch_size, seq_length, embed_dim)\n",
        "        K = self.W_k(x)  # (batch_size, seq_length, embed_dim)\n",
        "        V = self.W_v(x)  # (batch_size, seq_length, embed_dim)\n",
        "        d_k = K.shape[-1] # Key dimension\n",
        "\n",
        "\n",
        "        # Dot-product similarities\n",
        "        scores = Q @ K.transpose(1, 2)\n",
        "        # Scale by dimension\n",
        "        scores /= d_k ** 0.5\n",
        "        # Transform the scores into probabilities with the softmax function\n",
        "        scores = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # Optional: store the attention weights for visualization\n",
        "        self.attention_weights = scores\n",
        "\n",
        "        # Update the vectors x\n",
        "        x = scores @ V\n",
        "\n",
        "        return x\n",
        "\n",
        "# Testing\n",
        "attn = SingleHeadAttention(embed_dim=128)\n",
        "x = torch.randn(32, 10, 128)  # Batch of 32 sequences, each of length 10 with 128-d embeddings\n",
        "output = attn(x)\n",
        "print(output.shape)  # Should be (32, 10, 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG8Ps2w-GcFj"
      },
      "source": [
        "#### 2.2 Integrate the self-attention layer in a transformer layer\n",
        "\n",
        "A **Transformer Encoder Layer** consists of:  \n",
        "- A *self-attention mechanism* to capture long-range dependencies.  \n",
        "- *Fully connected (feedforward) layers* to transform representations.  \n",
        "- *Layer normalization* to stabilize training.  \n",
        "- *Residual connections* to improve gradient flow and prevent vanishing gradients.\n",
        "<div style=\"max-width:400px\">\n",
        "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5024bcc5-33c9-4d53-9bd7-56cbcf9c4627_874x1108.png\" alt=\"Transformer Layer\" />\n",
        "<div/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2QPI_RDGcFj",
        "outputId": "3727e635-caaf-417f-d3e7-45057ff44e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 10, 128])\n"
          ]
        }
      ],
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.self_attn = SingleHeadAttention(embed_dim)\n",
        "        # Normalization layers\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        # Fully connected layers\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_dim * 2, embed_dim)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x: (batch_size, seq_length, embed_dim)\n",
        "        # TODO: Implement encoder block, with residual connections!\n",
        "\n",
        "        x = x + self.self_attn(self.norm1(x))\n",
        "        x = x + self.fc_layers(self.norm2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "# Testing\n",
        "attn = TransformerEncoderLayer(embed_dim=128)\n",
        "x = torch.randn(32, 10, 128)  # Batch of 32 sequences, each of length 10 with 128-d embeddings\n",
        "output = attn(x)\n",
        "print(output.shape)  # Should be (32, 10, 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjz8NYegGcFl",
        "outputId": "8429d923-645a-4c26-9a9f-70d87db34f5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 10, 128])\n"
          ]
        }
      ],
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.self_attn = SingleHeadAttention(embed_dim)\n",
        "        # Normalization layers\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        # Fully connected layers\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_dim * 2, embed_dim)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x: (batch_size, seq_length, embed_dim)\n",
        "        # TODO: Implement encoder block, with residual connections!\n",
        "        y = x + self.self_attn(self.norm1(x))\n",
        "        x = self.fc_layers(self.norm2(y)) + y\n",
        "        #x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "# Testing\n",
        "attn = TransformerEncoderLayer(embed_dim=128)\n",
        "x = torch.randn(32, 10, 128)  # Batch of 32 sequences, each of length 10 with 128-d embeddings\n",
        "output = attn(x)\n",
        "print(output.shape)  # Should be (32, 10, 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-FEfY6nGcFn"
      },
      "source": [
        "---\n",
        "### 3. Implement a `Transformer` network\n",
        "\n",
        "#### 3.1. General architecture:\n",
        "The full Transformer network consists of:  \n",
        "1. **Embedding Module**: Converts input tokens into dense vectors and adds positional encodings.  \n",
        "2. **Transformer Layers**: A stack of self-attention layers with feedforward networks and normalization.  \n",
        "\n",
        "<div style=\"max-width:600px\">\n",
        "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6133c18-bfaf-4578-8c5a-e5ac7809f65b_1632x784.png\" alt=\"Transformer Architecture, with zoom on transformer layer\", \"width=\"750px\"\\>\n",
        "</div>\n",
        "\n",
        "3. **Classification Head**: Processes the output of the Transformer layers to produce predictions.\n",
        "\n",
        "#### 3.2. Predictions for our task\n",
        "\n",
        "The task is to **sort a list of integers**. What should be the output of the model? Of what dimension is it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjGcOjLUGcFn",
        "outputId": "2adfd758-221e-4f8d-a869-2d2d35d1f9de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence:         [13, 15, 15, 5, 12]\n",
            "Expected Sorted Output: [5, 12, 13, 15, 15]\n",
            "Model Prediction:       [18, 17, 7, 18, 13]\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, seq_length=5, num_layers=2):\n",
        "        \"\"\"\n",
        "        Transformer Encoder for sequence processing.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Number of unique tokens in the input vocabulary.\n",
        "            embed_dim (int): Dimension of the token embeddings.\n",
        "            num_layers (int): Number of Transformer encoder layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Token embedding layer\n",
        "        self.embedding = IntegerSequenceEmbedding(vocab_size, embed_dim, seq_length)\n",
        "\n",
        "        # Stack of Transformer Encoder Layers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(embed_dim) for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Final classification head: a simple linear layer\n",
        "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the Transformer Encoder.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (batch_size, seq_length).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor of shape (batch_size, seq_length, vocab_size) containing probabilities for each token.\n",
        "        \"\"\"\n",
        "        # Convert input sequence to embeddings\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Pass through Transformer Encoder Layers\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Apply final linear layer to get logits\n",
        "        outputs = self.fc_out(x)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "embed_dim = 32\n",
        "batch_size = 16\n",
        "transformer = Transformer(VOCAB_SIZE, embed_dim=embed_dim, seq_length=SEQ_LENGTH)\n",
        "# Generate source and target data\n",
        "source, target = generate_data(batch_size, SEQ_LENGTH, VOCAB_SIZE)\n",
        "\n",
        "# Pass the source data through the transformer and check the output shape\n",
        "outputs = transformer(source)\n",
        "predictions = outputs.argmax(dim=-1) # predictions should be a list of integers, the same length as source.\n",
        "print(f\"Input Sequence:         {source.tolist()[0]}\")\n",
        "print(f\"Expected Sorted Output: {target.tolist()[0]}\")\n",
        "print(f\"Model Prediction:       {predictions.tolist()[0]}\")\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtFW8kpeGcFq",
        "outputId": "0f86f54a-c2b4-46e8-8d22-c98079c4c781"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 5, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JA3YcD2GcFr"
      },
      "source": [
        "---\n",
        "### 4. Train the network!\n",
        "\n",
        "As for other neural networks, the Transformer parameters are learned by stochastic gradient descent on a training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdNT9GVgGcFs",
        "outputId": "2c8220dc-df10-4621-84ee-401ea886d630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Input Sequence:         [15, 9, 2, 15, 1, 12, 2, 17, 12, 9]\n",
            "Expected Sorted Output: [1, 2, 2, 9, 9, 12, 12, 15, 15, 17]\n",
            "Model Prediction:       [14, 5, 8, 10, 3, 8, 8, 15, 0, 11]\n",
            "Loss: 3.3911\n",
            "--------------------------------------------------\n",
            "Epoch 10\n",
            "Input Sequence:         [9, 17, 13, 14, 17, 17, 6, 1, 7, 9]\n",
            "Expected Sorted Output: [1, 6, 7, 9, 9, 13, 14, 17, 17, 17]\n",
            "Model Prediction:       [2, 3, 5, 8, 10, 10, 12, 13, 18, 19]\n",
            "Loss: 2.2553\n",
            "--------------------------------------------------\n",
            "Epoch 50\n",
            "Input Sequence:         [10, 9, 18, 14, 4, 17, 10, 19, 18, 11]\n",
            "Expected Sorted Output: [4, 9, 10, 10, 11, 14, 17, 18, 18, 19]\n",
            "Model Prediction:       [4, 9, 9, 10, 9, 14, 14, 17, 18, 19]\n",
            "Loss: 1.1029\n",
            "--------------------------------------------------\n",
            "Epoch 100\n",
            "Input Sequence:         [12, 12, 13, 6, 14, 13, 1, 3, 19, 5]\n",
            "Expected Sorted Output: [1, 3, 5, 6, 12, 12, 13, 13, 14, 19]\n",
            "Model Prediction:       [1, 3, 6, 6, 6, 13, 13, 14, 18, 19]\n",
            "Loss: 0.6489\n",
            "--------------------------------------------------\n",
            "Epoch 150\n",
            "Input Sequence:         [5, 17, 9, 3, 4, 13, 9, 8, 13, 16]\n",
            "Expected Sorted Output: [3, 4, 5, 8, 9, 9, 13, 13, 16, 17]\n",
            "Model Prediction:       [3, 4, 5, 9, 9, 9, 13, 13, 16, 17]\n",
            "Loss: 0.3989\n",
            "--------------------------------------------------\n",
            "Epoch 200\n",
            "Input Sequence:         [4, 12, 6, 5, 2, 3, 15, 15, 7, 16]\n",
            "Expected Sorted Output: [2, 3, 4, 5, 6, 7, 12, 15, 15, 16]\n",
            "Model Prediction:       [2, 4, 4, 6, 6, 7, 12, 15, 15, 16]\n",
            "Loss: 0.2716\n",
            "--------------------------------------------------\n",
            "Epoch 250\n",
            "Input Sequence:         [13, 2, 16, 1, 14, 4, 16, 14, 14, 9]\n",
            "Expected Sorted Output: [1, 2, 4, 9, 13, 14, 14, 14, 16, 16]\n",
            "Model Prediction:       [1, 2, 4, 9, 14, 14, 14, 14, 16, 16]\n",
            "Loss: 0.2853\n",
            "--------------------------------------------------\n",
            "Epoch 300\n",
            "Input Sequence:         [12, 4, 1, 7, 18, 13, 11, 15, 17, 18]\n",
            "Expected Sorted Output: [1, 4, 7, 11, 12, 13, 15, 17, 18, 18]\n",
            "Model Prediction:       [1, 4, 7, 11, 12, 13, 15, 17, 18, 18]\n",
            "Loss: 0.1588\n",
            "--------------------------------------------------\n",
            "Epoch 350\n",
            "Input Sequence:         [15, 6, 10, 11, 2, 15, 15, 14, 2, 1]\n",
            "Expected Sorted Output: [1, 2, 2, 6, 10, 11, 14, 15, 15, 15]\n",
            "Model Prediction:       [1, 2, 2, 6, 8, 11, 14, 15, 15, 15]\n",
            "Loss: 0.2271\n",
            "--------------------------------------------------\n",
            "Epoch 400\n",
            "Input Sequence:         [17, 3, 2, 8, 6, 12, 12, 4, 6, 12]\n",
            "Expected Sorted Output: [2, 3, 4, 6, 6, 8, 12, 12, 12, 17]\n",
            "Model Prediction:       [2, 3, 4, 6, 6, 8, 12, 12, 12, 17]\n",
            "Loss: 0.1542\n",
            "--------------------------------------------------\n",
            "Epoch 450\n",
            "Input Sequence:         [4, 8, 1, 18, 12, 3, 3, 5, 6, 18]\n",
            "Expected Sorted Output: [1, 3, 3, 4, 5, 6, 8, 12, 18, 18]\n",
            "Model Prediction:       [1, 3, 3, 5, 6, 6, 8, 12, 18, 18]\n",
            "Loss: 0.1239\n",
            "--------------------------------------------------\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "# Data\n",
        "vocab_size = 20\n",
        "seq_length = 10\n",
        "# Network hyperparameters\n",
        "embed_dim = 32\n",
        "num_layers = 2\n",
        "# Training hyperparameters\n",
        "batch_size = 32\n",
        "num_epochs = 500\n",
        "\n",
        "# Model, Loss, Optimizer\n",
        "model = Transformer(vocab_size, embed_dim=embed_dim, seq_length=seq_length)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Generate a mini-batch for training\n",
        "    src, tgt = generate_data(batch_size, seq_length, vocab_size)\n",
        "    # Forward pass\n",
        "    output = model(src)\n",
        "    loss = criterion(output.flatten(0,1), tgt.flatten()) # Why flatten the output?\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Parameter updates\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print model output at each epoch\n",
        "    if epoch % 50 == 0 or epoch==10:\n",
        "        test_src, test_tgt = generate_data(batch_size, seq_length, vocab_size)\n",
        "        # test_pred should be a list of integers, the same length as test_src.\n",
        "        test_pred = model(test_src).argmax(dim=-1)\n",
        "\n",
        "        print(f\"Epoch {epoch}\")\n",
        "        print(f\"Input Sequence:         {test_src.tolist()[0]}\")\n",
        "        print(f\"Expected Sorted Output: {test_tgt.tolist()[0]}\")\n",
        "        print(f\"Model Prediction:       {test_pred.tolist()[0]}\")\n",
        "        print(f\"Loss: {loss.item():.4f}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81WB-ZwJGcFs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXRJYilcGcFt",
        "outputId": "e071e02a-ab97-4d3e-c746-48683836fb8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence 1:         [4, 8, 1, 18, 12, 3, 3, 5, 6, 18]\n",
            "Expected Sorted Output 1: [1, 3, 3, 4, 5, 6, 8, 12, 18, 18]\n",
            "Model Prediction 1:       [1, 3, 3, 5, 6, 6, 8, 12, 18, 18]\n",
            "--------------------------------------------------\n",
            "Input Sequence 2:         [13, 12, 16, 16, 19, 5, 3, 6, 10, 11]\n",
            "Expected Sorted Output 2: [3, 5, 6, 10, 11, 12, 13, 16, 16, 19]\n",
            "Model Prediction 2:       [3, 5, 6, 10, 11, 12, 13, 16, 16, 19]\n",
            "--------------------------------------------------\n",
            "Input Sequence 3:         [1, 16, 8, 3, 13, 8, 1, 17, 5, 11]\n",
            "Expected Sorted Output 3: [1, 1, 3, 5, 8, 8, 11, 13, 16, 17]\n",
            "Model Prediction 3:       [1, 3, 3, 5, 8, 8, 11, 13, 16, 17]\n",
            "--------------------------------------------------\n",
            "Input Sequence 4:         [18, 9, 3, 17, 14, 15, 13, 9, 17, 10]\n",
            "Expected Sorted Output 4: [3, 9, 9, 10, 13, 14, 15, 17, 17, 18]\n",
            "Model Prediction 4:       [3, 9, 9, 10, 13, 14, 15, 17, 17, 18]\n",
            "--------------------------------------------------\n",
            "Input Sequence 5:         [7, 15, 2, 17, 9, 2, 5, 12, 17, 5]\n",
            "Expected Sorted Output 5: [2, 2, 5, 5, 7, 9, 12, 15, 17, 17]\n",
            "Model Prediction 5:       [2, 2, 5, 5, 7, 9, 12, 15, 17, 17]\n",
            "--------------------------------------------------\n",
            "Loss: 0.1582\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test the model on 5 example sequences\n",
        "for i in range(5):\n",
        "    print(f\"Input Sequence {i + 1}:         {test_src.tolist()[i]}\")\n",
        "    print(f\"Expected Sorted Output {i + 1}: {test_tgt.tolist()[i]}\")\n",
        "    print(f\"Model Prediction {i + 1}:       {test_pred.tolist()[i]}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(f\"Loss: {loss.item():.4f}\")\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRgCoY9ZGcFt"
      },
      "source": [
        "---\n",
        "### 5. Implement multi-headed attention\n",
        "\n",
        "<div style=\"max-width:400px\">\n",
        "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F65c156ae-5cc5-4f7f-8652-dd5311b19beb_544x724.png\" alt=\"Transformer Architecture, with zoom on transformer layer\", \"width=\"50px\"\\>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "UUmHOQhoGcFu"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert embed_dim % num_heads == 0, \"Embedding size must be divisible by num_heads\"\n",
        "\n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim)  # Query projection\n",
        "        self.W_k = nn.Linear(embed_dim, embed_dim)  # Key projection\n",
        "        self.W_v = nn.Linear(embed_dim, embed_dim)  # Value projection\n",
        "        self.fc_out = nn.Linear(embed_dim, embed_dim)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, embed_dim = x.shape\n",
        "\n",
        "        # TODO: Compute Queries, Keys, Values\n",
        "        Q = self.W_q(x)  # (batch_size, seq_length, embed_dim)\n",
        "        K = self.W_k(x)  # (batch_size, seq_length, embed_dim)\n",
        "        V = self.W_v(x)  # (batch_size, seq_length, embed_dim)\n",
        "\n",
        "        # Reshape for multiple heads\n",
        "        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # TODO: Apply scaled dot-product attention\n",
        "        # Dot-product similarities\n",
        "        scores = Q @ K.transpose(-2, -1)\n",
        "        # Scale by key dimension\n",
        "        scores /=  K.shape[-1] ** 0.5\n",
        "        # Transform the scores into probabilities with the softmax function\n",
        "        scores = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # Calculate attention output\n",
        "        attn_output = scores @ V  # (batch_size, num_heads, seq_length, head_dim)\n",
        "\n",
        "        # Reshape and apply final linear layer\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_dim)\n",
        "        output = self.fc_out(attn_output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(embed_dim, num_heads)\n",
        "        # Normalization layers\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        # Fully connected layers\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_dim * 2, embed_dim)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x: (batch_size, seq_length, embed_dim)\n",
        "        # TODO: Implement encoder block, with residual connections!\n",
        "\n",
        "        x = x + self.self_attn(self.norm1(x))\n",
        "        x = x + self.fc_layers(self.norm2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "# Testing\n",
        "attn = TransformerEncoderLayer(embed_dim=128, num_heads=4)\n",
        "x = torch.randn(32, 10, 128)  # Batch of 32 sequences, each of length 10 with 128-d embeddings\n",
        "output = attn(x)\n",
        "print(output.shape)  # Should be (32, 10, 128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qhxd8QUzKyrq",
        "outputId": "69a652ad-8bcf-4dde-cb70-909eabaf3487"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 10, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, seq_length=5, num_layers=2, num_heads=8): # Added num_heads here\n",
        "        \"\"\"\n",
        "        Transformer Encoder for sequence processing.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Number of unique tokens in the input vocabulary.\n",
        "            embed_dim (int): Dimension of the token embeddings.\n",
        "            num_layers (int): Number of Transformer encoder layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Token embedding layer\n",
        "        self.embedding = IntegerSequenceEmbedding(vocab_size, embed_dim, seq_length)\n",
        "\n",
        "        # Stack of Transformer Encoder Layers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(embed_dim, num_heads) for i in range(num_layers) # Pass num_heads here\n",
        "        ])\n",
        "\n",
        "        # Final classification head: a simple linear layer\n",
        "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the Transformer Encoder.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (batch_size, seq_length).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor of shape (batch_size, seq_length, vocab_size) containing probabilities for each token.\n",
        "        \"\"\"\n",
        "        # Convert input sequence to embeddings\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Pass through Transformer Encoder Layers\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Apply final linear layer to get logits\n",
        "        outputs = self.fc_out(x)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "embed_dim = 32\n",
        "batch_size = 16\n",
        "transformer = Transformer(VOCAB_SIZE, embed_dim=embed_dim, seq_length=SEQ_LENGTH)\n",
        "# Generate source and target data\n",
        "source, target = generate_data(batch_size, SEQ_LENGTH, VOCAB_SIZE)\n",
        "\n",
        "# Pass the source data through the transformer and check the output shape\n",
        "outputs = transformer(source)\n",
        "predictions = outputs.argmax(dim=-1) # predictions should be a list of integers, the same length as source.\n",
        "print(f\"Input Sequence:         {source.tolist()[0]}\")\n",
        "print(f\"Expected Sorted Output: {target.tolist()[0]}\")\n",
        "print(f\"Model Prediction:       {predictions.tolist()[0]}\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ymtc3hkPKync",
        "outputId": "c0502fd9-0b3c-4b93-e0e3-d4d90429662f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence:         [1, 17, 8, 2, 8]\n",
            "Expected Sorted Output: [1, 2, 8, 8, 17]\n",
            "Model Prediction:       [14, 14, 13, 8, 7]\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "vocab_size = 20\n",
        "seq_length = 10\n",
        "# Network hyperparameters\n",
        "embed_dim = 32\n",
        "num_layers = 2\n",
        "# Training hyperparameters\n",
        "batch_size = 32\n",
        "num_epochs = 500\n",
        "\n",
        "# Model, Loss, Optimizer\n",
        "model = Transformer(vocab_size, embed_dim=embed_dim, seq_length=seq_length)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Generate a mini-batch for training\n",
        "    src, tgt = generate_data(batch_size, seq_length, vocab_size)\n",
        "    # Forward pass\n",
        "    output = model(src)\n",
        "    loss = criterion(output.flatten(0,1), tgt.flatten()) # Why flatten the output?\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Parameter updates\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print model output at each epoch\n",
        "    if epoch % 50 == 0 or epoch==10:\n",
        "        test_src, test_tgt = generate_data(batch_size, seq_length, vocab_size)\n",
        "        # test_pred should be a list of integers, the same length as test_src.\n",
        "        test_pred = model(test_src).argmax(dim=-1)\n",
        "\n",
        "        print(f\"Epoch {epoch}\")\n",
        "        print(f\"Input Sequence:         {test_src.tolist()[0]}\")\n",
        "        print(f\"Expected Sorted Output: {test_tgt.tolist()[0]}\")\n",
        "        print(f\"Model Prediction:       {test_pred.tolist()[0]}\")\n",
        "        print(f\"Loss: {loss.item():.4f}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "print(\"Training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pT0F0HnLe1N",
        "outputId": "f9601021-1636-4e81-e9c2-85c9624f6d78"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Input Sequence:         [19, 14, 13, 18, 15, 18, 9, 18, 15, 19]\n",
            "Expected Sorted Output: [9, 13, 14, 15, 15, 18, 18, 18, 19, 19]\n",
            "Model Prediction:       [14, 4, 3, 7, 14, 13, 3, 11, 2, 5]\n",
            "Loss: 3.3361\n",
            "--------------------------------------------------\n",
            "Epoch 10\n",
            "Input Sequence:         [8, 18, 14, 1, 13, 3, 8, 19, 9, 7]\n",
            "Expected Sorted Output: [1, 3, 7, 8, 8, 9, 13, 14, 18, 19]\n",
            "Model Prediction:       [1, 4, 5, 8, 8, 11, 14, 14, 17, 18]\n",
            "Loss: 2.1849\n",
            "--------------------------------------------------\n",
            "Epoch 50\n",
            "Input Sequence:         [19, 5, 1, 12, 19, 17, 16, 9, 9, 1]\n",
            "Expected Sorted Output: [1, 1, 5, 9, 9, 12, 16, 17, 19, 19]\n",
            "Model Prediction:       [1, 1, 5, 9, 9, 9, 12, 17, 17, 19]\n",
            "Loss: 0.9719\n",
            "--------------------------------------------------\n",
            "Epoch 100\n",
            "Input Sequence:         [11, 11, 1, 14, 16, 3, 15, 7, 7, 2]\n",
            "Expected Sorted Output: [1, 2, 3, 7, 7, 11, 11, 14, 15, 16]\n",
            "Model Prediction:       [1, 2, 3, 7, 7, 11, 11, 14, 14, 16]\n",
            "Loss: 0.5340\n",
            "--------------------------------------------------\n",
            "Epoch 150\n",
            "Input Sequence:         [4, 11, 10, 10, 15, 14, 10, 8, 16, 8]\n",
            "Expected Sorted Output: [4, 8, 8, 10, 10, 10, 11, 14, 15, 16]\n",
            "Model Prediction:       [4, 8, 8, 10, 10, 11, 14, 15, 15, 16]\n",
            "Loss: 0.3611\n",
            "--------------------------------------------------\n",
            "Epoch 200\n",
            "Input Sequence:         [17, 6, 8, 13, 5, 2, 15, 1, 8, 8]\n",
            "Expected Sorted Output: [1, 2, 5, 6, 8, 8, 8, 13, 15, 17]\n",
            "Model Prediction:       [1, 2, 5, 6, 8, 8, 11, 13, 15, 17]\n",
            "Loss: 0.2276\n",
            "--------------------------------------------------\n",
            "Epoch 250\n",
            "Input Sequence:         [14, 15, 10, 15, 17, 17, 8, 13, 17, 6]\n",
            "Expected Sorted Output: [6, 8, 10, 13, 14, 15, 15, 17, 17, 17]\n",
            "Model Prediction:       [6, 8, 10, 13, 14, 15, 15, 17, 17, 17]\n",
            "Loss: 0.1910\n",
            "--------------------------------------------------\n",
            "Epoch 300\n",
            "Input Sequence:         [12, 15, 13, 9, 13, 3, 8, 4, 10, 11]\n",
            "Expected Sorted Output: [3, 4, 8, 9, 10, 11, 12, 13, 13, 15]\n",
            "Model Prediction:       [3, 4, 8, 10, 10, 11, 12, 13, 13, 15]\n",
            "Loss: 0.1428\n",
            "--------------------------------------------------\n",
            "Epoch 350\n",
            "Input Sequence:         [19, 1, 1, 5, 13, 11, 11, 8, 17, 10]\n",
            "Expected Sorted Output: [1, 1, 5, 8, 10, 11, 11, 13, 17, 19]\n",
            "Model Prediction:       [1, 1, 5, 8, 10, 11, 11, 13, 17, 19]\n",
            "Loss: 0.1558\n",
            "--------------------------------------------------\n",
            "Epoch 400\n",
            "Input Sequence:         [9, 8, 10, 6, 18, 15, 8, 3, 10, 13]\n",
            "Expected Sorted Output: [3, 6, 8, 8, 9, 10, 10, 13, 15, 18]\n",
            "Model Prediction:       [3, 6, 8, 8, 9, 10, 10, 13, 15, 18]\n",
            "Loss: 0.0538\n",
            "--------------------------------------------------\n",
            "Epoch 450\n",
            "Input Sequence:         [6, 11, 14, 9, 18, 7, 3, 8, 5, 17]\n",
            "Expected Sorted Output: [3, 5, 6, 7, 8, 9, 11, 14, 17, 18]\n",
            "Model Prediction:       [3, 5, 6, 7, 8, 9, 11, 14, 17, 18]\n",
            "Loss: 0.0580\n",
            "--------------------------------------------------\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model on 5 example sequences\n",
        "for i in range(5):\n",
        "    print(f\"Input Sequence {i + 1}:         {test_src.tolist()[i]}\")\n",
        "    print(f\"Expected Sorted Output {i + 1}: {test_tgt.tolist()[i]}\")\n",
        "    print(f\"Model Prediction {i + 1}:       {test_pred.tolist()[i]}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(f\"Loss: {loss.item():.4f}\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQelVYYsLexQ",
        "outputId": "270e0ab5-83c3-435a-e1b3-24284bccd397"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence 1:         [6, 11, 14, 9, 18, 7, 3, 8, 5, 17]\n",
            "Expected Sorted Output 1: [3, 5, 6, 7, 8, 9, 11, 14, 17, 18]\n",
            "Model Prediction 1:       [3, 5, 6, 7, 8, 9, 11, 14, 17, 18]\n",
            "--------------------------------------------------\n",
            "Input Sequence 2:         [6, 7, 7, 17, 5, 9, 3, 4, 9, 4]\n",
            "Expected Sorted Output 2: [3, 4, 4, 5, 6, 7, 7, 9, 9, 17]\n",
            "Model Prediction 2:       [3, 4, 4, 4, 6, 7, 7, 9, 9, 17]\n",
            "--------------------------------------------------\n",
            "Input Sequence 3:         [2, 8, 18, 18, 10, 2, 12, 1, 9, 8]\n",
            "Expected Sorted Output 3: [1, 2, 2, 8, 8, 9, 10, 12, 18, 18]\n",
            "Model Prediction 3:       [1, 2, 2, 8, 8, 9, 10, 12, 18, 18]\n",
            "--------------------------------------------------\n",
            "Input Sequence 4:         [7, 2, 17, 13, 3, 12, 19, 13, 1, 12]\n",
            "Expected Sorted Output 4: [1, 2, 3, 7, 12, 12, 13, 13, 17, 19]\n",
            "Model Prediction 4:       [1, 2, 3, 7, 12, 12, 13, 13, 17, 19]\n",
            "--------------------------------------------------\n",
            "Input Sequence 5:         [14, 8, 10, 8, 1, 5, 7, 3, 7, 2]\n",
            "Expected Sorted Output 5: [1, 2, 3, 5, 7, 7, 8, 8, 10, 14]\n",
            "Model Prediction 5:       [1, 2, 3, 5, 7, 7, 7, 8, 10, 14]\n",
            "--------------------------------------------------\n",
            "Loss: 0.0975\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FtB7ODGkLehv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}